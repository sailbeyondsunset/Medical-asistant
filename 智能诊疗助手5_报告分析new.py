import torch
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import HumanMessage, AIMessage
from typing import Any, List, Optional

from langchain_community.document_loaders import PyPDFLoader
from PIL import Image
import pytesseract

# å‘é‡æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
embedding_model_dir = snapshot_download('AI-ModelScope/bge-large-zh-v1.5', cache_dir='./')

# æºå¤§æ¨¡å‹ä¸‹è½½
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')
#model_dir = snapshot_download('IEITYuan/Yuan2-2B-July-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
model_path = './IEITYuan/Yuan2-2B-Mars-hf'
#model_path = './IEITYuan/Yuan2-2B-July-hf'

# å®šä¹‰å‘é‡æ¨¡å‹è·¯å¾„
embedding_model_path = './AI-ModelScope/bge-large-zh-v1___5'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16

# å®šä¹‰æºå¤§æ¨¡å‹ç±»
class Yuan2_LLM(LLM):
    """
    class for Yuan2_LLM
    """
    tokenizer: AutoTokenizer = None
    model: AutoModelForCausalLM = None

    def __init__(self, model_path: str):
        super().__init__()
        
        try:        # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
            print("Creat tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
            self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

            print("Creat model...")
            self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()
        except Exception as e:
            print(f"Error loading model or tokenizer: {e}")
            
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        prompt = prompt.strip()
        prompt += "<sep>"
        inputs = self.tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
        outputs = self.model.generate(inputs, do_sample=False, max_length=4096)
        output = self.tokenizer.decode(outputs[0])
        #output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = output.split("<sep>")[-1].split("<eod>")[0]

        return response

    @property
    def _llm_type(self) -> str:
        return "Yuan2_LLM"

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–llmå’Œembeddings
@st.cache_resource
def get_models():
    llm = Yuan2_LLM(model_path)

    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True}  # è®¾ç½®ä¸ºTrueä»¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    embeddings = HuggingFaceEmbeddings(
        model_name=embedding_model_path,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
    )
    return llm, embeddings

#2. å‘æ‚£è€…æé—®ä»¥è·å–æ›´å¤šæœ‰åŠ©äºè¯Šæ–­çš„ä¿¡æ¯ï¼šä¾‹å¦‚ç—‡çŠ¶çš„å¼€å§‹æ—¶é—´ã€æŒç»­æ—¶é—´ã€ç–¼ç—›ç¨‹åº¦ã€æ˜¯å¦æœ‰å…¶ä»–ä¼´éšç—‡çŠ¶ã€æ—¢å¾€ç—…å²ã€å®¶æ—ç—…å²ç­‰ã€‚ #ï¼šåŒ…æ‹¬ç–¾ç—…çš„å¸¸è§ç—‡çŠ¶ã€å¯èƒ½çš„ç—…å› ã€‚ï¼Œå¹¶è¯´æ˜æ¯ç§æ²»ç–—æ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹ã€‚    è¯·ç¡®ä¿æ¶µç›–ä»¥ä¸‹å‡ ç‚¹ï¼š
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”æ£€æŸ¥æŠ¥å‘Šåˆ†æâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
Analyze_template = """
å‡è®¾ä½ æ˜¯ä¸€ä¸ªæ£€éªŒåŒ»ç”Ÿï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ£€æŸ¥æŠ¥å‘Šå†…å®¹ï¼Œåˆ†ä»¥ä¸‹å‡ ç‚¹ç»™å‡ºä¸€äº›åˆæ­¥çš„è¯Šæ–­å»ºè®®ï¼š
1. æœ€å¯èƒ½çš„ä¸¤ç§ç–¾ç—…è¯Šæ–­åŠè¯Šæ–­ç†ç”±ï¼Œç®€å•åˆ—å‡ºæ¯ç§ç–¾ç—…çš„å¸¸è§ç—‡çŠ¶ã€‚
2. éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥çš„é¡¹ç›®ã€‚
3. ç®€å•è§£é‡Šå¯èƒ½çš„æ²»ç–—æ–¹æ¡ˆã€‚
4. å…·ä½“çš„ç”Ÿæ´»æ–¹å¼æˆ–é¥®é£Ÿæ–¹é¢çš„å»ºè®®ã€‚

    æ£€æŸ¥æŠ¥å‘Šå†…å®¹å¦‚ä¸‹ï¼š
    {report_text}
"""
# è¯Šæ–­çš„promptæ¨¡æ¿
diagnose_prompt_template = """
å‡è®¾æ‚¨æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œã€å°Šé‡å’Œè¯šå®çš„åŒ»ç”ŸåŠ©ç†ã€‚è¯·åŸºäºæ‚£è€…æƒ…å†µå’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. ç»™å‡ºæœ€å¯èƒ½çš„ç–¾ç—…è¯Šæ–­åŠè¯Šæ–­ç†ç”±ï¼Œç®€å•åˆ—å‡ºæ¯ç§ç–¾ç—…çš„å¸¸è§ç—‡çŠ¶ã€å¯èƒ½çš„ç—…å› ã€‚
2. ç»™å‡ºé€‚åˆçš„å°±è¯Šç§‘å®¤ï¼Œå»ºè®®éœ€è¦è¿›è¡Œçš„æ£€æŸ¥é¡¹ç›®ã€‚
3. ç®€å•è§£é‡Šå¯èƒ½çš„æ²»ç–—æ–¹æ¡ˆï¼Œ
4. ç»™å‡ºå…·ä½“çš„ç”Ÿæ´»å»ºè®®ï¼Œä»¥å¸®åŠ©æ‚£è€…æ›´å¥½åœ°ç®¡ç†å¥åº·çŠ¶å†µã€‚

æ‚£è€…æƒ…å†µå’ŒèƒŒæ™¯ä¿¡æ¯å¦‚ä¸‹: 
{situation_and_context}

å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·å‘æ‚£è€…æé—®ä»¥è·å–æ›´å¤šæœ‰åŠ©äºè¯Šæ–­çš„ä¿¡æ¯ï¼šä¾‹å¦‚ç—‡çŠ¶çš„æŒç»­æ—¶é—´ã€ç–¼ç—›ç¨‹åº¦ã€æ˜¯å¦æœ‰å…¶ä»–ä¼´éšç—‡çŠ¶ã€æ—¢å¾€ç—…å²ã€å®¶æ—ç—…å²ç­‰ã€‚
"""

# æ€»ç»“è¯Šæ–­å’Œå»ºè®®çš„promptæ¨¡æ¿
conclude_prompt_template = """
å‡è®¾æ‚¨æ˜¯ä¸€ä½ä¹äºåŠ©äººã€å¯é ã€å°Šé‡å’Œè¯šå®çš„åŒ»ç”ŸåŠ©ç†ã€‚è¯·åŸºäºå¯¹è¯è®°å½•å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. æ˜ç¡®ç»™å‡ºæœ€å¯èƒ½çš„ç–¾ç—…è¯Šæ–­ã€‚
2. é’ˆå¯¹è¯Šæ–­ï¼Œå»ºè®®éœ€è¦è¿›è¡Œçš„å…·ä½“æ£€æŸ¥é¡¹ç›®ã€‚
3. æä¾›å…·ä½“å¯è¡Œçš„æ²»ç–—æ–¹æ¡ˆï¼Œéè¯ç‰©æ²»ç–—çš„è¯¦ç»†æ­¥éª¤ã€‚
4. ç»™å‡ºé¥®é£Ÿã€è¿åŠ¨ã€ä½œæ¯ç­‰æ–¹é¢çš„ç”Ÿæ´»å»ºè®®ï¼Œç®€å•è¯´æ˜æ¯é¡¹å»ºè®®å¯¹ç–¾ç—…ç®¡ç†çš„ä½œç”¨ã€‚

å¯¹è¯è®°å½•: {conversation}
"""

class IntelligentDiagnosticAssistant:
    # å®šä¹‰IntelligentDiagnosticAssistantç±»
    def __init__(self, llm, embeddings,file_path):
        #1. å¯¼å…¥å‚æ•°&è°ƒç”¨æ–¹æ³•åˆå§‹åŒ–
        self.llm = llm
        self.embeddings = embeddings
        self.file_path = file_path
        self.memory = ConversationBufferMemory(memory_key="conversation", return_messages=True) #æ¶‰åŠå¯¹è¯å†å²
        
        #2. åŠ è½½ text_splitter ç”¨äºå°†æ–‡æœ¬åˆ‡åˆ†æˆè¾ƒå°çš„å—
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=450,#,100
            chunk_overlap=10,#5
            length_function=len
        )
        
        #3. ç»™å‡ºä¸åŒprompt
        self.analyze_prompt = PromptTemplate(input_variables= ["report_text"], template=Analyze_template)
        self.diagnose_prompt = PromptTemplate(input_variables=["situation_and_context"], template=diagnose_prompt_template)
        self.conclude_prompt = PromptTemplate(input_variables=["conversation"], template=conclude_prompt_template)
        
        #4. ç”¨äºåŠ è½½é—®ç­”é“¾
        self.analyze_chain = LLMChain(llm=self.llm, prompt=self.analyze_prompt)
        self.diagnosis_chain = LLMChain(llm=self.llm, prompt=self.diagnose_prompt, memory=self.memory)
        self.diagnosis_conclu_chain = LLMChain(llm=self.llm, prompt=self.conclude_prompt)
        
    #5. å®šä¹‰embeddingå‡½æ•°ï¼Œå°†åŒ»ç–—è¯­æ–™åº“è½¬æˆå‘é‡
    def vectors(self,file_path):
        # è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå°†æ¯ä¸€è¡Œåˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€å­—ç¬¦ä¸²
        with open(self.file_path, 'r', encoding='utf-8') as file:
            self.documents = file.read()  # å°†æ•´ä¸ªæ–‡ä»¶å†…å®¹ä½œä¸ºå•ä¸€å­—ç¬¦ä¸²
        # åˆ‡åˆ†æˆchunks
        all_chunks = self.text_splitter.split_text(text=self.documents)
        # è½¬æˆå‘é‡å¹¶å­˜å‚¨
        vector_store = FAISS.from_texts(all_chunks, embedding=self.embeddings)
        return vector_store  # æ·»åŠ è¿”å›ï¼Œä¸éœ€è¦é‡å¤åŠ è½½
    
    #6. æ£€ç´¢ç›¸ä¼¼çš„ chunks
    def get_context(self,vector_store,symptoms): 
        retrieved_chunks = vector_store.similarity_search(query=symptoms, k=2)
        return retrieved_chunks
    
    #7. ä¾æ®æƒ…å†µç”Ÿæˆå›å¤    
        # æ£€æŸ¥æŠ¥å‘Šåˆ†æ
    def analyze_report(self, docs):
        results = self.analyze_chain.run({"report_text": docs})
        return results
    
    def get_diagnosis(self, dialog_history, context):
        # åˆå¹¶å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²
        combined_input = "<n>".join([f"{msg['role']}: {msg['content']}" for msg in dialog_history])
        combined_input += f"<n> èƒŒæ™¯ä¿¡æ¯: {context}"
        result = self.diagnosis_chain.run({"situation_and_context": combined_input})
        return result

    def diagnosis_conclu(self, dialog_history ):
        # ä»è®°å¿†ä¸­è·å–å¯¹è¯å†…å®¹
        conversation = "<n>".join([f"{msg['role']}: {msg['content']}" for msg in dialog_history])
        # ç¡®ä¿ conversation ä¸æ˜¯ç©ºçš„
        if not conversation:
            raise ValueError("ç”Ÿæˆè¯Šç–—å»ºè®®æ—¶å‡ºç°é”™è¯¯: å¯¹è¯è®°å½•ä¸ºç©ºæˆ–æœªåŠ è½½æ­£ç¡®ã€‚")
        # ç”Ÿæˆæœ€ç»ˆè¯Šæ–­
        result = self.diagnosis_conclu_chain.run({"conversation": conversation})
        return result


def main():
        
    # åˆ›å»ºä¸€ä¸ªæ ‡é¢˜
    st.title('ğŸ‘©â€âš•ï¸ æ™ºèƒ½å¥åº·å°åŠ©æ‰‹')
    # è·å–llmå’Œembeddings
    if 'llm' not in st.session_state or 'embeddings' not in st.session_state:
        try:
            st.session_state.llm, st.session_state.embeddings = get_models()
        except Exception as e:
            st.error(f"åŠ è½½æ¨¡å‹æ—¶å‡ºç°é”™è¯¯: {e}")
            return
    #file_path = "./1_2MedicalQ_A.txt" #2_DiseasesData.txt
    file_path = "./4_diseases_infoDXYS.txt"
        
    # åˆå§‹åŒ–æ™ºèƒ½è¯Šæ–­åŠ©æ‰‹
    assistant = IntelligentDiagnosticAssistant(
        st.session_state.llm,
        st.session_state.embeddings,
        file_path
    )

    if 'dialog_history' not in st.session_state:
        st.session_state.dialog_history = []
        
    if 'vectors' not in st.session_state:
        try:
            with st.spinner('æ­£åœ¨åŠ è½½çŸ¥è¯†åº“ï¼Œè¯·ç¨å€™...'):
                st.session_state.vectors = assistant.vectors(file_path)
                st.success('çŸ¥è¯†åº“åŠ è½½æˆåŠŸï¼')
        except Exception as e:
            st.error(f"åˆå§‹åŒ–å‘é‡å­˜å‚¨æ—¶å‡ºç°é”™è¯¯: {e}")
            return 
     #â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”è¯Šæ–­æŠ¥å‘Šâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    uploaded_file = st.file_uploader("ä¸Šä¼ æ£€æŸ¥æŠ¥å‘Š", type=["pdf", "png", "jpg", "jpeg"], help="ä¸Šä¼ æ‚¨çš„æ£€æŸ¥æŠ¥å‘Šæ–‡ä»¶ï¼ˆPDFæˆ–å›¾ç‰‡ï¼‰")
    if uploaded_file :
        file_type = uploaded_file.type
        file_content = uploaded_file.read()
        
        if file_type == "application/pdf":
            st.write("æ‚¨ä¸Šä¼ äº†ä¸€ä¸ªPDFæ–‡ä»¶")
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä¿å­˜ä¸Šä¼ çš„PDFæ–‡ä»¶
            temp_file_path = "temp.pdf"
            with open(temp_file_path, "wb") as temp_file:
                temp_file.write(file_content)
            # åŠ è½½ä¸´æ—¶æ–‡ä»¶ä¸­çš„å†…å®¹
            loader = PyPDFLoader(temp_file_path)
            docs = loader.load()
            # æå–å†…å®¹
            content = ""
            for doc in docs:
                content += doc.page_content  # å‡è®¾æ¯ä¸ª doc å¯¹è±¡æœ‰ page_content å±æ€§
        else:
            st.write("æ‚¨ä¸Šä¼ äº†ä¸€ä¸ªå›¾ç‰‡æ–‡ä»¶")
            image = Image.open(uploaded_file)
            content = pytesseract.image_to_string(image)
        
        # ç”Ÿæˆæ¦‚æ‹¬
        Analysis = assistant.analyze_report(content)
        st.write(Analysis)

    # èŠå¤©è®°å½•å±•ç¤º
    with st.container():
        for message in st.session_state.dialog_history:
            st.chat_message(message['role']).write(message['content'])

    # è¯Šæ–­,å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ï¼Œå¹¶åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤º
    if user_input:= st.text_input("æˆ‘æ˜¯æ‚¨çš„ä¸“å±å¥åº·åŠ©æ‰‹ï¼Œè¯·æè¿°æ‚¨çš„ç—‡çŠ¶æˆ–ä¸é€‚ï¼Œæˆ‘å°†æä¾›å¥åº·å»ºè®®", help="è¾“å…¥åæŒ‰ Enter é”®æäº¤ã€‚"):
        st.session_state.dialog_history.append({"role": "user", "content": user_input})
        st.chat_message("user").write(user_input)
        st.chat_message("assistant").write(f"æ­£åœ¨åˆ†ææ‚¨çš„æƒ…å†µï¼Œä¸ºæ‚¨æä¾›ä¸“å±å»ºè®®ï¼Œè¯·ç¨å€™...")        #st.spinner('æ­£åœ¨åˆ†ææ‚¨çš„ç—‡çŠ¶ï¼Œè¯·ç¨å€™...')

        try: #ç”¨å¯¹è¯å†å²å’Œcontextè¿›è¡Œåˆ†æ
            # åˆ‡å‰²å¯¹è¯å†å²
            max_history_length = 6  # ä¿ç•™æœ€è¿‘6æ¡å¯¹è¯
            relevant_history = st.session_state.dialog_history[-max_history_length:]
            
            #è·å–èƒŒæ™¯çŸ¥è¯†
            context = assistant.get_context(st.session_state.vectors, user_input)

            # ç”Ÿæˆè¯Šæ–­
            diagnosis = assistant.get_diagnosis(relevant_history, context)
            
            st.session_state.dialog_history.append({"role": "assistant", "content": diagnosis})
            st.chat_message("assistant").write(diagnosis)
       
        except Exception as e:
            st.error(f"å¤„ç†è¯Šæ–­æ—¶å‡ºç°é”™è¯¯: {e}")

    # ä¾§è¾¹æ æ·»åŠ â€œç”Ÿæˆç»¼åˆè¯Šç–—å»ºè®®â€
    with st.sidebar:
        if st.button("ç”Ÿæˆç»¼åˆè¯Šç–—å»ºè®®"):
            st.chat_message("assistant").write('æ­£åœ¨ç”Ÿæˆç»¼åˆè¯Šç–—å»ºè®®ï¼Œè¯·ç¨å€™...')
            #st.spinner('æ­£åœ¨ç”Ÿæˆç»¼åˆè¯Šç–—å»ºè®®ï¼Œè¯·ç¨å€™...')
            try:
                diagnosis_conclusion = assistant.diagnosis_conclu(st.session_state.dialog_history)
                st.sidebar.write("ç»¼åˆè¯Šç–—å»ºè®®:")
                st.sidebar.write(diagnosis_conclusion)
                st.sidebar.write("ä»¥ä¸Šå»ºè®®ä»…ä½œä¸ºå‚è€ƒï¼Œå¦‚æœç—‡çŠ¶æŒç»­æˆ–åŠ é‡ï¼Œè¯·å’¨è¯¢åŒ»ç”Ÿã€‚")
            except ValueError as e:
                st.error(f"ç”Ÿæˆç»¼åˆè¯Šç–—å»ºè®®æ—¶å‡ºç°é”™è¯¯: {e}")

if __name__ == '__main__':
    main()
    
# åˆ å»åˆæ¬¡å¯¹è¯çš„å·®åˆ«ï¼Œç›´æ¥ç”¨å†å²èµ„æ–™+contextæ‹¼æ¥